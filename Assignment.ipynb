{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MVA DL Assignment 1\n",
    "\n",
    "# 1. Variational Autoencoders\n",
    "\n",
    "## 1.2 Decoders\n",
    "\n",
    "### Question 1 [3 points]\n",
    "Describe the steps needed to sample from such a model.\n",
    "\n",
    "*Answer:* We sample from the described model by sampling a latent variable $z$ from a zero-mean unit-variance Gaussian. We then input $z$ to the neural network $f_{\\theta}$ to obtain $M$ Bernoulli parameters. We then obtain our image by sampling each pixel as 0 or 1 depending on the Bernoulli parameter $p_m$ for that pixel.\n",
    "\n",
    "### Question 2 [3 points] \n",
    "Although Monte-Carlo Integration with samples from $p(z_n)$ can be used to approximate $\\log p(x_n)$, it is not used for training VAE type of models, because it is inefficient. In a few sentences, describe why it is inefficient and how this efficiency scales with the dimensionality of z.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1.3 KL Divergence\n",
    "\n",
    "### Question 3 [3 points] \n",
    "Assume that $q$ and $p$ in Eq. 8, are univariate gaussians: $q = N (\\mu_q , \\sigma^2_q )$ and $p = N (\\mu_p, \\sigma^2_p )$. Give two examples of $(\\mu_q , \\mu_p, \\sigma^2_q , \\sigma^2_p )$: one of which results in a very small, and one of which has a very large, KL-divergence: $DKL(q||p)$.\n",
    "\n",
    "*Answer:* let $ p ~ N(0,1)$ and $q~ N(1,a)$. Then \n",
    "\n",
    "$$KL(p||q) = \\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{1}{2} x^2) (\\frac{1}{2}(x-a)^2-x^2)dx \\\\\n",
    " = E_p[\\frac{1}{2}((x-a)^2-x^2)]\\\\\n",
    " = \\frac{1}{2}a^2$$\n",
    "\n",
    " So for $ a = 0.1$, we get $KP(p||q) = 0.005$, while if $a=10$, we get $KL(p||q) = 50$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " ## The Encoder\n",
    "\n",
    " ### Question 4 [3 points] \n",
    " Explain how you can see from Eq. 16 that the right hand side had to be a lower bound of the log probability $\\log p(x_n)$. Why must we optimize the lower-bound, instead of optimizing the log-probability $\\log p(x_n)$ directly?\n",
    "\n",
    " *Response:* Because KL divergence is always non-negative, we see immediately that the log likelihood is greater than the right hand side. We optimize the lower bound because it is computationally tractable, in our generative model, as opposed to the direct integral.\n",
    " \n",
    " ### Question 5 [3 points]\n",
    "Now, looking at the two terms on left-hand side of Eq. 16: Two things can happen when the lower-bound is pushed up. Can you describe what these two thing are?\n",
    "\n",
    "*Answer:* We can either increase the log likelihood of our data, which means we have learned a probabilistic model that better fits our data distribution, or we decrease the divergence between our model's latent prior and the actual latent prior, thereby ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1.5 Specifying the Encoder\n",
    "\n",
    "### Question 6 [4 points] \n",
    "The loss in Eq. 17 can be rewritten in terms of per-sample losses:\n",
    "$$L = \\frac{1}{N} \\sum_{n=1}^N (\\mathbf{L}_n^{\\text{recon}}+ \\mathbf{L}^{\\text{reg}}_n ) $$\n",
    "where $ \\mathbf{L}_n^{\\text{recon}} =  −E_{q_{\\phi}(z|x_n)}[\\log p_{\\theta} (x_n|Z)]$ and $\\mathbf{L}^{\\text{reg}}_n = $D_{KL}(Q(Z|x_n)||p_{\\theta} (Z))$ can be seen as a reconstruction loss term and\n",
    "an regularization term, respectively. Explain why the names reconstruction and regularization are appropriate for these two\n",
    "losses. (Hint: Suppose we use just one sample to approximate the expectation $E_{\\phi} (z|x_n )[p_{\\theta} (n|Z)]$ – as is common practice in VAEs.)\n",
    "\n",
    "*Response:* The reconstruction term measures how likely our original data point $x_n$ is under the distribution reconstructed from the latent code $Z$ using our learned parameters $\\theta$. Thus it measures how well our model can reconstruct the distribution that our data comes from given the latent code.\n",
    "\n",
    "The regularization term comes from quantifies the divergence of our actual latent distribution created by $q_{\\phi}$ from our desired Guassian prior $p(z)$.\n",
    "\n",
    "### Question 7 [4 points] \n",
    "Now we have defined an objective (Eq. 17) in terms of an abstract model and variational approximation, we can put everything together using our model definition (Equations 1, 2) and definition of $q_{\\theta} (z_n|x_n)$ (Eq. 18), and we can write down a single objective which we can minimize. Write down expressions (including steps) for $\\mathbf{L}^{\\text{recon}}_n$ and $\\mathbf{L}^{\\text{reg}}_n$ such that we can minimize $L = \\frac{1}{N} \\sum_{n=1}^N (\\mathbf{L}_n^{\\text{recon}}+ \\mathbf{L}^{\\text{reg}}_n ) $ as our final objective. Make any approximation explicit.\n",
    "\n",
    "*Response:* \n",
    "- Sample data point at random from data set\n",
    "- Use encoder neural net to get parameters of $q_{\\phi}$.\n",
    "- Use equation 9 to calculate KL divergence.\n",
    "- Use decoder network to get distribution $p(\\cdot|Z)$, which we then can use to evaluate likelihood of original data point.\n",
    "- Use backpropagation to calculate the weight updates for parameters $\\theta$ and $\\phi$ to minimize loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1.6 Reparametrization\n",
    "\n",
    "### Question 8 [4 points] \n",
    "Passing the derivative through samples can be done using the reparameterization trick. In a few sentences, explain why the act of sampling usually prevents us from computing $\\nabla_{\\phi}L$, and how the reparameterization trick solves this problem. (Hint: you can take a look at Figure 4 from the tutorial by Carl Doersch)\n",
    "\n",
    "*Response:* The problem with trying to differentiate $L$ with respect to $\\phi$ directly is that... By reparameterizing $Z$ as $\\mu_{\\phi} (x_n) +sigma_{\\phi} \\epsilon$, with $\\epsilon ~ N(0,1)$, we can shift the randomness from the parameters $\\phi$. This then allows us to directly calculate the gradient with respect to $\\phi$ by ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1.7 Building a VAE\n",
    "\n",
    "### Question 9 [4 points] \n",
    "Build a Variational Autoencoder, and train it on the binarized MNIST dataset. Both the encoder and decoder should be implemented as an MLP. Following standard practice – and for simplicity – you may assume that\n",
    "the number of samples used to approximate the expectation in Lrecon\n",
    "n is 1. Use a latent space size of $z_{\\text{dim}} = 20$. In your report, provide a short description (no more than 10 lines) of the used architectures for the encoder and decoder, any hyperparameters and your training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2_mean = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc2_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.relu(self.fc1(x))\n",
    "        return self.fc2_mean(h), self.fc2_logvar(h)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = self.relu(self.fc3(z))\n",
    "        return self.sigmoid(self.fc4(h))\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(input_dim, hidden_dim, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim, input_dim)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z), mu, logvar\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "# Load binarized MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), lambda x: (x > 0.5).float()])\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Initialize VAE, optimizer, and loss function\n",
    "input_dim = 28 * 28\n",
    "hidden_dim = 400\n",
    "latent_dim = 20\n",
    "vae = VAE(input_dim, hidden_dim, latent_dim)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "vae.train()\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.view(-1, input_dim)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = vae(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}, Loss: {train_loss / len(train_loader.dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Architecture: Antisymmetric 2-layer MLPs\n",
    "- Hyperparameters:\n",
    "- Training Steps: Adam, batch size, 10 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10 [4 points] \n",
    "Plot 64 samples (8 × 8grid) from your model at three points throughout training (before training, after training 10 epochs, and after training 80 epochs). You should observe an improvement in the quality of samples. Describe shortly the quality and/or issues of the generated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(vae, epoch, num_samples=64):\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples, latent_dim)\n",
    "        samples = vae.decoder(z).view(-1, 28, 28).cpu().numpy()\n",
    "\n",
    "    fig, axes = plt.subplots(8, 8, figsize=(8, 8))\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        ax.imshow(samples[i], cmap='gray')\n",
    "        ax.axis('off')\n",
    "    plt.suptitle(f'Samples at Epoch {epoch}')\n",
    "    plt.show()\n",
    "\n",
    "# Plot initial samples before training\n",
    "plot_samples(vae, epoch=0)# Load binarized MNIST dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), lambda x: (x > 0.5).float()])\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 80\n",
    "vae.train()\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.view(-1, input_dim)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = vae(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch == 10 or epoch == 80:\n",
    "        plot_samples(vae, epoch=epoch)\n",
    "    \n",
    "    print(f'Epoch {epoch}, Loss: {train_loss / len(train_loader.dataset)}')\n",
    "\n",
    "# Plot final samples after training\n",
    "plot_samples(vae, epoch=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the code, you should observe the generated samples at three points: before training, after 10 epochs, and after 80 epochs. Here is a short analysis of the quality and issues of the generated images:\n",
    "\n",
    "- Before Training: The initial samples are likely to be random noise, as the model has not yet learned any meaningful representations.\n",
    "- After 10 Epochs: The samples should start to show some structure, but they may still be blurry or lack detail. The model is beginning to learn the underlying data distribution.\n",
    "- After 80 Epochs: The samples should be much clearer and more detailed. The model has had sufficient time to learn the data distribution, and the generated images should resemble the training data more closely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Diffusion\n",
    "\n",
    "### Question 11 [4 points] \n",
    "Using $\\log p(x) = \\log(\\int p(x, z_{1:T} )dz_{1:T} )$, extend the ELBO formula for Markovian Hierarchical Variational Autoencoder to obtain : \n",
    "$$\\log p(x) \\geq E_{q_{\\phi}(z_{1:T} |x)} \\left[\\log \\frac{p(x,z_{1:T} )}{q_{\\phi}(z_{1:T} |x)}\\right]$$\n",
    "\n",
    "*Response:* Using the definition of KL divergence, we see that \n",
    "$$\\log p(x) = E_{q_{\\phi}(z_{1:T} |x)} \\left[\\log \\frac{p(x,z_{1:T} )}{q_{\\phi}(z_{1:T} |x)}\\right] + \\text{KL}(q_{\\phi}(z_{1:T} |x)||p_{\\theta}(z_{1:T} |x))\\\\\n",
    "= E_{q_{\\phi}(z_{1:T} |x)} \\left[\\log \\frac{p(x,z_{1:T} )}{p_{\\theta}(z_{1:T} |x)}\\right]\\\\\n",
    "= E_{q_{\\phi}(z_{1:T} |x)} \\left[\\log p(x)\\right]\n",
    "$$\n",
    "\n",
    "### Question 12 [4 points] \n",
    "Based on these 3 restrictions, describe in a few lines the process (architecture, transformations, inputs and outputs).\n",
    "\n",
    "*Response*: \n",
    "- Architecture: Since each step of the diffusion process has the same dimension, we can reuse the same architecture with same input and output dimension and an additional input for time step to define the reverse diffusion process.\n",
    "- The forward transformation is simply a scaling of the input and an addition of Gaussian noise with an amplitude determined by the noising schedule.\n",
    "- The input of the forward process is an image, and the output is a significantly noised and rescaled version of that image\n",
    "- The input of the backwards process in standard Gaussian noise, and the output is an image resembling an image in the dataset.\n",
    "\n",
    "### Question 13 [3 points]\n",
    "\n",
    "Comment on the several terms of the formula in eq. 24. What is the role of each of these terms in the ELBO formula?\n",
    "\n",
    "*Response*\n",
    "- Reconstruction Term: This term simply ensures that our final decoder layer, which is different from the others since $\\log p_{\\theta} (z_0|z_1)$ must be a discrete distribution, has a good distribution for the final denoised image that puts high likelihood on real images when conditioned on their noised versions.  \n",
    "- Prior Matching Term: This term says that our total denoising process should result in $z_T$ being as close to our prior $P(z_T)$ of pure Gaussian noise as possible.\n",
    "- Consistency Terms: These terms ensure that our Gaussian denoisers $p_{\\theta}(z_t|z_{t+1})$ are good models for the noising process $q_{\\phi}(z_t|z_{t-1}).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Question 14 [4 points] \n",
    "Using the reparametrization trick and eq. 23, we can write that:\n",
    "$$z_t = \\sqrt{\\alpha_t} z_{t−1} + \\sqrt{1 − \\alpha_t} \\epsilon$$\n",
    " with $\\epsilon ~ N (\\epsilon; 0, I)$$\n",
    "Show that z_t follow a distribution that can be written as eq. 26.\n",
    "$$z_t ∼ N (z_{t-1}; \\sqrt{\\bar{\\alpha}_t}z_0, (1 − \\bar{\\alpha})I)\n",
    "\n",
    "*Response:*  We proceed with proof by induction: It can be seen using formula 25 directly that the base case \n",
    "$t=1$ holds.\n",
    "\n",
    "For the inductive step, we note that if \n",
    "$$z_t = \\sqrt{\\bar{\\alpha_t} z_{0}} + \\sqrt{1 − \\bar{\\alpha_t}} \\epsilon$$\n",
    "then\n",
    "$$z_{t+1} = \\sqrt{\\bar{\\alpha}_{t+1}} z_{0} + \\sqrt{\\alpha_{t+1}} \\sqrt{1 − \\bar{\\alpha_t}} \\epsilon + \\sqrt{1-\\alpha_{t+1}} \\epsilon'\\\\\n",
    "= \\sqrt{\\bar{\\alpha}_{t+1}} z_{0} + \\sqrt{\\alpha_{t+1} − \\bar{\\alpha}_{t+1} + 1-\\alpha_{t+1}} \\epsilon\\\\\n",
    "= \\sqrt{\\bar{\\alpha}_{t+1}} z_{0} + \\sqrt{1 − \\bar{\\alpha}_{t+1} } \\epsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 15 [4 points] \n",
    "Plug equations 29 and 30 in equation 28 and find an interpretation to the final optimization term.\n",
    "\n",
    "*response:* plugging in gives us \n",
    "$$\\arg \\min_{\\theta} D_{\\text{KL}} (q(z_{t−1}|z_t, z_0)||p\\theta (z_{t−1}|z_t)) = \\arg \\min_{\\theta} \\frac{\\bar{\\alpha}_{t-1} (1-\\alpha_t)^2}{2\\sigma_q^2(t)(1-\\bar{\\alpha_t})^2}||z_0-\\hat{z_{\\theta}}(z_t, t)||^2$$\n",
    "\n",
    "With this parameterization, one can think of the final optimization term as forcing the network to learn a good approximation of the added Gaussian noise depending on the noised image $z_t$ and the timestep $t$.\n",
    "\n",
    "### Question 16 [4 points] \n",
    "Recall that $q(z_t|z_0) is a Gaussian of form $N (z_t; \\sqrt{\\bar{\\alpha}_t}z_0,(1 − \\bar{\\alpha}_t)I). Then, following the definition\n",
    "of the signal-to-noise ratio (SNR) as $\\text{SNR} = \\frac{\\mu^2}{\\sigma^2} , deduce the SNR at timestep $t$.\n",
    "\n",
    "Given this, we have $\\text{SNR} = \\frac{\\bar{\\alpha}}{1-\\bar{\\alpha}} z_0^{\\top}z_0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Question 17 [4 points] \n",
    "Introduce the SNR into the equation that you got at Question 15 (recall that $\\sigma^2_q(t) = \\frac{(1−\\alpha_t) (1−\\bar{\\alpha}_{t−1})}{1−\\bar{\\alpha}_t}$.)\n",
    "What does it mean for our diffusion model? Link it to the definition of the SNR.\n",
    "\n",
    "*Response:* Plugging in the definition of $\\sigma^2_q(t)$ and SNR to the equation derived in question 15 gives that \n",
    "$$\\arg \\min_{\\theta} D_{\\text{KL}} (q(z_{t−1}|z_t, z_0)||p\\theta (z_{t−1}|z_t)) = \\arg \\min_{\\theta} \\frac{\\text{SNR}_{t-1} (1-\\alpha_t)}{2(1-\\bar{\\alpha_t})}||z_0-\\hat{z_{\\theta}}(z_t, t)||^2$$\n",
    "\n",
    "This means for the diffusion model that..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Diffusion in Practice\n",
    "\n",
    "### Question 18 [4 points] \n",
    "The reverse diffusion process (ie. generating an image from noise) can be applied with multiple\n",
    "ways of sampling the denoised image at each timestep t. For example it is possible to skip some timesteps to fasten the generation process, or to change the amount of noise added to an image to a different schedule to reduce the variations at the end of the diffusion process. The goal is to look at the practical details of diffusion by changing the scheduler when generating an image.\n",
    "\n",
    "Find a way to generate images with the same seed but with different number of steps and different kind of noise schedulers. Plot a grid of generated images with different number of steps as rows and different schedulers as columns (use at least three schedulers including Euler, Euler Ancestral and DPM). What can you say about the differences between schedulers? What is the impact of the number of steps?\n",
    "\n",
    "Hint: you can look at the tutorial https://huggingface.co/docs/diffusers/using-diffusers/schedulers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import DiffusionPipeline, DDIMScheduler, EulerDiscreteScheduler, EulerAncestralDiscreteScheduler, DPMSolverMultistepScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the schedulers\n",
    "schedulers = {\n",
    "    'DDIM': DDIMScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\"),\n",
    "    'Euler': EulerDiscreteScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\"),\n",
    "    'Euler Ancestral': EulerAncestralDiscreteScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\"),\n",
    "    'DPM': DPMSolverMultistepScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")\n",
    "}\n",
    "\n",
    "# Load the pipeline\n",
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    \"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float16, use_safetensors=True\n",
    ").to(\"cuda\")\n",
    "\n",
    "prompt = \"A photograph of an astronaut riding a horse on Mars, high resolution, high definition.\"\n",
    "generator = torch.Generator(device=\"cuda\").manual_seed(8)\n",
    "\n",
    "# Generate images\n",
    "def generate_images(pipeline, schedulers, steps_list, prompt, generator):\n",
    "    images = {}\n",
    "    for name, scheduler in schedulers.items():\n",
    "        images[name] = []\n",
    "        for steps in steps_list:\n",
    "            scheduler.set_timesteps(steps)\n",
    "            pipeline.scheduler = scheduler\n",
    "            with torch.no_grad():\n",
    "                image = pipeline(prompt, num_inference_steps=steps, generator=generator).images[0]\n",
    "            images[name].append(image)\n",
    "    return images\n",
    "\n",
    "# Plot the grid\n",
    "def plot_grid(images, steps_list):\n",
    "    fig, axes = plt.subplots(len(steps_list), len(images), figsize=(10, 10))\n",
    "    for i, steps in enumerate(steps_list):\n",
    "        for j, (name, imgs) in enumerate(images.items()):\n",
    "            axes[i, j].imshow(imgs[i])\n",
    "            if i == 0:\n",
    "                axes[i, j].set_title(name)\n",
    "            if j == 0:\n",
    "                axes[i, j].set_ylabel(f'Steps: {steps}')\n",
    "            axes[i, j].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Parameters\n",
    "steps_list = [10, 20, 50, 100]\n",
    "\n",
    "# Generate and plot images\n",
    "images = generate_images(pipeline, schedulers, steps_list, prompt, generator)\n",
    "plot_grid(images, steps_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 19 [4 points]\n",
    "Use a pre-trained diffusion model that can be guided by a textual prompt and generate an image\n",
    "with a given prompt (eg. ‘a photograph of an astronaut riding a horse’). Guidance can also be used in a negative manner\n",
    "to remove something from generated images (eg. hands with 6 fingers, bad weather, undesired objects...).\n",
    "\n",
    "Using the same initial noise and prompt as before, find a way to remove an object/a color/a concept that is present in\n",
    "your generated image but not in your prompt (e.g. if you generate an image without specifying the background and you\n",
    "get a desert but you want something else, remove the desert by using a negative prompt ‘desert’ like in Fig. 2).\n",
    "\n",
    "Plot the two images side by side, along with their prompt/negative prompt, and write a small paragraph analyzing\n",
    "them. What can you say about the position of the object?\n",
    "\n",
    "Hint: you can use the Diffusers library and modify the line\n",
    "`noise pred = noise pred uncond + guidance scale *\n",
    "(noise_pred_text−noisepreduncond)`\n",
    "in the section ‘Deconstruct the Stable Diffusion pipeline’ from the previous url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the pre-trained diffusion model\n",
    "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "# Define the prompts\n",
    "prompt = \"A photograph of an astronaut riding a horse on Mars, high resolution, high definition.\"\n",
    "negative_prompt = \"desert\"\n",
    "\n",
    "# Set the generator for reproducibility\n",
    "generator = torch.Generator(device=\"cuda\").manual_seed(8)\n",
    "\n",
    "# Generate image with the prompt\n",
    "image_with_prompt = pipeline(prompt, num_inference_steps=50, generator=generator).images[0]\n",
    "\n",
    "# Generate image with the prompt and negative prompt\n",
    "image_with_negative_prompt = pipeline(prompt, negative_prompt=negative_prompt, num_inference_steps=50, generator=generator).images[0]\n",
    "\n",
    "# Plot the images side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "axes[0].imshow(image_with_prompt)\n",
    "axes[0].set_title(\"Prompt: \" + prompt)\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(image_with_negative_prompt)\n",
    "axes[1].set_title(\"Prompt: \" + prompt + \"\\nNegative Prompt: \" + negative_prompt)\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Multilingual Translation\n",
    "\n",
    "## 3.1 Prenormalization\n",
    "\n",
    "### Question 20. [13 points] \n",
    "Derive analytically the impact of that different sequence of layers, when doing back-propagation during training. (Hint: If you denote by $e$ the error produced in one pass, and $x_L$ the top-most layer, then by applying chain rule you obtain:)\n",
    "$$ \\frac{\\partial e }{\\partial x_l}= \\frac{\\partial e}{\\partial x_L} \\frac{\\partial x_L}{\\partial x_l}$$\n",
    "Derive then $\\frac{\\partial x_{l+1}}{\\partial x_l}$. (no need of actually deriving $\\frac{\\partial LayerNorm(x)}{\\partial x}$ ).\n",
    "\n",
    "$$ \\frac{\\partial e }{\\partial x_l}= \\frac{\\partial e}{\\partial x_L} \\Pi_{k=l}^{L-1}\\frac{\\partial x_{k+1}}{\\partial x_k}$$\n",
    "\n",
    "For eq 31, this gives\n",
    "\n",
    "$$ \\frac{\\partial e }{\\partial x_l}= \\frac{\\partial e}{\\partial x_L} \\Pi_{k=l}^{L-1}\\left(LayerNorm'(x_k+A(x_l))(1+A'(x_l))\\right)$$\n",
    "\n",
    "For eq 32, this gives \n",
    "\n",
    "$$ \\frac{\\partial e }{\\partial x_l}= \\frac{\\partial e}{\\partial x_L} \\Pi_{k=l}^{L-1}\\left(1 + A'(LayerNorm(x_k))LayerNorm'(x_k)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Question 21. [8.5 points] \n",
    "Explain in a few paragraphs what do you think are the consequences of training (very deep\n",
    "models) of those derivations when using Eq 32 instead of Eq. 31?\n",
    "\n",
    "For very deep networks, it seems that the gradient might be easier to control, so that it could be easier to avoid the problem of vanishing or exploding gradients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training deep transformer blocks, the placement of Layer Normalization (LayerNorm) can significantly impact the model's performance and stability. Here are the differences between applying LayerNorm before the attention layer and applying it last to the updated embeddings:\n",
    "\n",
    "### Applying LayerNorm Before the Attention Layer\n",
    "\n",
    "This approach is often referred to as **Pre-LayerNorm**. In this setup, LayerNorm is applied to the input embeddings before they are fed into the attention layer and the feed-forward network.\n",
    "\n",
    "**Advantages:**\n",
    "1. **Stabilizes Training**: Pre-LayerNorm can help stabilize training, especially in very deep transformer models, by normalizing the input to each layer, which can prevent the gradients from exploding or vanishing.\n",
    "2. **Improved Gradient Flow**: By normalizing the inputs to the attention and feed-forward layers, it can improve the gradient flow through the network, making it easier to train deeper models.\n",
    "\n",
    "**Disadvantages:**\n",
    "1. **Potentially Slower Convergence**: In some cases, Pre-LayerNorm can lead to slower convergence compared to Post-LayerNorm.\n",
    "\n",
    "### Applying LayerNorm Last to the Updated Embeddings\n",
    "\n",
    "This approach is often referred to as **Post-LayerNorm**. In this setup, LayerNorm is applied to the output of the attention layer and the feed-forward network, after the residual connections have been added.\n",
    "\n",
    "**Advantages:**\n",
    "1. **Faster Convergence**: Post-LayerNorm can sometimes lead to faster convergence during training.\n",
    "2. **Direct Residual Learning**: By applying LayerNorm after the residual connections, the model can directly learn the residuals, which can be beneficial for certain tasks.\n",
    "\n",
    "**Disadvantages:**\n",
    "1. **Training Instability**: Post-LayerNorm can sometimes lead to training instability, especially in very deep transformer models, as the inputs to each layer are not normalized, which can cause issues with gradient flow.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Question 22. [8.5 points] \n",
    "Show empirically the impact of this. This is, design a measure and plot that measure against\n",
    "training steps for a standard task (eg: language modelling), comparing those plots between a deep and shallow model.\n",
    "Finding what exactly to measure is an important part of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerBlockPreLayerNorm(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim):\n",
    "        super(TransformerBlockPreLayerNorm, self).__init__()\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layernorm1(x)\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        x = x + attn_output\n",
    "        x = self.layernorm2(x)\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = x + ff_output\n",
    "        return x\n",
    "\n",
    "class TransformerBlockPostLayerNorm(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim):\n",
    "        super(TransformerBlockPostLayerNorm, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        x = x + attn_output\n",
    "        x = self.layernorm1(x)\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = x + ff_output\n",
    "        x = self.layernorm2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To empirically show the difference between Pre-LayerNorm and Post-LayerNorm, you can design an experiment that measures and compares the training stability and convergence speed of both normalization strategies on a standard task like language modeling. Here's a step-by-step guide to achieve this:\n",
    "\n",
    "1. **Define the Experiment**:\n",
    "   - Use a standard language modeling dataset (e.g., WikiText-2).\n",
    "   - Implement both Pre-LayerNorm and Post-LayerNorm transformer models.\n",
    "   - Train both models on the dataset.\n",
    "   - Measure training loss and validation loss over training steps.\n",
    "\n",
    "2. **Implement the Models**:\n",
    "   - Implement transformer models with Pre-LayerNorm and Post-LayerNorm.\n",
    "   - Implement both shallow and deep versions of these models.\n",
    "\n",
    "3. **Train the Models**:\n",
    "   - Train the models and log the training and validation loss at regular intervals.\n",
    "\n",
    "4. **Plot the Results**:\n",
    "   - Plot the training and validation loss against training steps for both normalization strategies.\n",
    "   - Compare the plots for shallow and deep models.\n",
    "\n",
    "Here's an example implementation:\n",
    "\n",
    "### Step 1: Define the Experiment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the dataset and data loader\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_iter = WikiText2(split='train')\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "def data_process(raw_text_iter):\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "train_data = data_process(WikiText2(split='train'))\n",
    "val_data = data_process(WikiText2(split='valid'))\n",
    "\n",
    "class WikiTextDataset(Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) // self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.seq_len\n",
    "        end = start + self.seq_len + 1\n",
    "        return self.data[start:end]\n",
    "\n",
    "seq_len = 35\n",
    "batch_size = 20\n",
    "\n",
    "train_dataset = WikiTextDataset(train_data, seq_len)\n",
    "val_dataset = WikiTextDataset(val_data, seq_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Step 2: Implement the Models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlockPreLayerNorm(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim):\n",
    "        super(TransformerBlockPreLayerNorm, self).__init__()\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layernorm1(x)\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        x = x + attn_output\n",
    "        x = self.layernorm2(x)\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = x + ff_output\n",
    "        return x\n",
    "\n",
    "class TransformerBlockPostLayerNorm(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim):\n",
    "        super(TransformerBlockPostLayerNorm, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        x = x + attn_output\n",
    "        x = self.layernorm1(x)\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = x + ff_output\n",
    "        x = self.layernorm2(x)\n",
    "        return x\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, ff_dim, num_layers, pre_layer_norm=True):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlockPreLayerNorm(embed_dim, num_heads, ff_dim) if pre_layer_norm else TransformerBlockPostLayerNorm(embed_dim, num_heads, ff_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Step 3: Train the Models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        data, targets = batch[:, :-1].to(device), batch[:, 1:].to(device)\n",
    "        output = model(data)\n",
    "        loss = criterion(output.view(-1, output.size(-1)), targets.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            data, targets = batch[:, :-1].to(device), batch[:, 1:].to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output.view(-1, output.size(-1)), targets.view(-1))\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def run_experiment(embed_dim, num_heads, ff_dim, num_layers, pre_layer_norm, epochs=10):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = TransformerModel(len(vocab), embed_dim, num_heads, ff_dim, num_layers, pre_layer_norm).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss = evaluate(model, val_loader, criterion, device)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Parameters\n",
    "embed_dim = 128\n",
    "num_heads = 4\n",
    "ff_dim = 512\n",
    "epochs = 10\n",
    "\n",
    "# Shallow model\n",
    "num_layers_shallow = 2\n",
    "train_losses_pre_shallow, val_losses_pre_shallow = run_experiment(embed_dim, num_heads, ff_dim, num_layers_shallow, pre_layer_norm=True, epochs=epochs)\n",
    "train_losses_post_shallow, val_losses_post_shallow = run_experiment(embed_dim, num_heads, ff_dim, num_layers_shallow, pre_layer_norm=False, epochs=epochs)\n",
    "\n",
    "# Deep model\n",
    "num_layers_deep = 6\n",
    "train_losses_pre_deep, val_losses_pre_deep = run_experiment(embed_dim, num_heads, ff_dim, num_layers_deep, pre_layer_norm=True, epochs=epochs)\n",
    "train_losses_post_deep, val_losses_post_deep = run_experiment(embed_dim, num_heads, ff_dim, num_layers_deep, pre_layer_norm=False, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Step 4: Plot the Results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(train_losses_pre, val_losses_pre, train_losses_post, val_losses_post, title):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(train_losses_pre, label='Train Loss (Pre-LayerNorm)')\n",
    "    plt.plot(val_losses_pre, label='Val Loss (Pre-LayerNorm)')\n",
    "    plt.plot(train_losses_post, label='Train Loss (Post-LayerNorm)')\n",
    "    plt.plot(val_losses_post, label='Val Loss (Post-LayerNorm)')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot results for shallow model\n",
    "plot_results(train_losses_pre_shallow, val_losses_pre_shallow, train_losses_post_shallow, val_losses_post_shallow, 'Shallow Model (2 Layers)')\n",
    "\n",
    "# Plot results for deep model\n",
    "plot_results(train_losses_pre_deep, val_losses_pre_deep, train_losses_post_deep, val_losses_post_deep, 'Deep Model (6 Layers)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Analysis\n",
    "\n",
    "After running the experiments and plotting the results, you can analyze the differences between Pre-LayerNorm and Post-LayerNorm for both shallow and deep models. Here are some points to consider:\n",
    "\n",
    "- **Training Stability**: Observe if one normalization strategy leads to more stable training, especially in the deep model.\n",
    "- **Convergence Speed**: Compare the speed of convergence for both strategies.\n",
    "- **Final Performance**: Evaluate the final training and validation loss to see which strategy performs better.\n",
    "\n",
    "By comparing these plots, you can empirically demonstrate the differences between Pre-LayerNorm and Post-LayerNorm in terms of training stability and convergence speed for both shallow and deep transformer models.\n",
    "\n",
    "Similar code found with 2 license types"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
